{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bd8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##denpendents \n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy, categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b91f36",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2519047073.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mRAW_DIR1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/data/raw\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "RAW_DIR1 = \"/content/drive/MyDrive/data/raw\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca14b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "##folder structures and directory\n",
    "RAW_DIR = \"data/raw\"\n",
    "OUT_DIR = \"data\"\n",
    "BAD_DIR = \"data/corrupt\"\n",
    "\n",
    "##image resize process\n",
    "IMG_HEIGHT=180    ##Resize all images\n",
    "IMG_WIDTH=180   ##Resize all images\n",
    "CHANNELS=3\n",
    "BATCH_SIZE=32 \n",
    "EPOCHS=10\n",
    "\n",
    "##splitting ratio\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "##Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76476756",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Instead of deleting immediately, we can move it\n",
    "#Move corrupted images to a separate folder \n",
    "\n",
    "def move_corrupted_images(image_dir, bad_dir):\n",
    "    os.makedirs(bad_dir, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(image_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                path = os.path.join(root, file)\n",
    "                try:\n",
    "                    img = Image.open(path)\n",
    "                    img.verify()\n",
    "                except:\n",
    "                    shutil.move(path, os.path.join(bad_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6cd78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##move the corrupted files\n",
    "move_corrupted_images(RAW_DIR,BAD_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6bb172",
   "metadata": {},
   "source": [
    "## Data Loading using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a0e8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory /Users/sarojrai/Desktop/Computer Vision/Plant Diseases Detection/data/raw",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1624716483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##data loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m raw_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"/Users/sarojrai/Desktop/Computer Vision/Plant Diseases Detection/data/raw\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_WIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /Users/sarojrai/Desktop/Computer Vision/Plant Diseases Detection/data/raw"
     ]
    }
   ],
   "source": [
    "##data loading \n",
    "raw_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"\",\n",
    "    shuffle=True,\n",
    "    image_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "##class name\n",
    "class_name=raw_dataset.class_names\n",
    "len(class_name)\n",
    "class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e91108",
   "metadata": {},
   "source": [
    "<h2 style='text-align:center; font-family:Geogeria; color:white'><b>Hardcode: Splitting Raw Data</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27133c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Identify class labels\n",
    "# folder_name_list=os.listdir(RAW_DIR)\n",
    "\n",
    "# ##Folder names become class labels\n",
    "# ##keeps only folders [\"apple\", \"mango\", \"grapes\", \"potato\"]\n",
    "# classes = [cls for cls in folder_name_list if os.path.isdir(os.path.join(RAW_DIR, cls))] \n",
    "\n",
    "# # Create directory structure\n",
    "# ##Create output directory structure\n",
    "# ##Iterates over dataset splits\n",
    "# for split in [\"train\", \"val\", \"test\"]:\n",
    "#     ##Iterates over class labels\n",
    "#     for clss in classes:\n",
    "#         os.makedirs(os.path.join(OUT_DIR, split, clss), exist_ok=True) #Creates directories\n",
    "\n",
    "# # Split per class\n",
    "# #Process one class at a time\n",
    "# #This ensures class-wise stratification\n",
    "# for cls in classes:\n",
    "#     #full path to one class folder like data/raw/apple\n",
    "#     cls_path = os.path.join(RAW_DIR, cls)\n",
    "#     ##list of images within list of folder \n",
    "#     img_list=os.listdir(cls_path)\n",
    "#     #Collects only image files\n",
    "#     images = [f for f in os.listdir(cls_path) if os.path.isfile(os.path.join(cls_path, f))]\n",
    "\n",
    "#     ##image suffling\n",
    "#     random.shuffle(images)\n",
    "\n",
    "#     #Compute split sizes\n",
    "#     n_total = len(images)\n",
    "#     n_train = int(TRAIN_RATIO * n_total)\n",
    "#     n_val = int(VAL_RATIO * n_total)\n",
    "\n",
    "#     train_imgs = images[:n_train]\n",
    "#     val_imgs = images[n_train:n_train + n_val]\n",
    "#     test_imgs = images[n_train + n_val:]\n",
    "\n",
    "#     #Copy images into split folders\n",
    "#     for img in train_imgs:\n",
    "#         shutil.copy(\n",
    "#             os.path.join(cls_path, img),\n",
    "#             os.path.join(OUT_DIR, \"train\", cls, img)\n",
    "#         )\n",
    "\n",
    "#     for img in val_imgs:\n",
    "#         shutil.copy(\n",
    "#             os.path.join(cls_path, img),\n",
    "#             os.path.join(OUT_DIR, \"val\", cls, img)\n",
    "#         )\n",
    "\n",
    "#     for img in test_imgs:\n",
    "#         shutil.copy(\n",
    "#             os.path.join(cls_path, img),\n",
    "#             os.path.join(OUT_DIR, \"test\", cls, img)\n",
    "#         )\n",
    "\n",
    "# print(\"Train / Validation / Test split completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1421ac7",
   "metadata": {},
   "source": [
    "<h2 style='text-align:center; font-family:Geogeria; color:white'><b>Data Preprocessing: Converting images into an Array</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##data directories\n",
    "# TRAIN_DIR='data/train'\n",
    "# TEST_DIR='data/test'\n",
    "# VAL_DIR='data/val'\n",
    "\n",
    "# # Loads images from directories, automatically assigns labels from folder names\n",
    "# ##output dataset format look like this (images,labels) in memory\n",
    "# train_ds = tf.keras.utils.image_dataset_from_directory( \n",
    "#     TRAIN_DIR,\n",
    "#     image_size=(IMG_HIGHT,IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,       # Shuffle training dataset\n",
    "#     seed=42,\n",
    "#     validation_split=False\n",
    "# )\n",
    "\n",
    "# ##validation data preprocess\n",
    "# val_ds = tf.keras.utils.image_dataset_from_directory( \n",
    "#     VAL_DIR,\n",
    "#     image_size=(IMG_HIGHT,IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,       #shuffle is not needed for validation/test\n",
    "#     validation_split=False\n",
    "# )\n",
    "\n",
    "# ##test data preprocess\n",
    "# test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     TEST_DIR,\n",
    "#     image_size=(IMG_HIGHT,IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,    #shuffle is not needed for validation/test\n",
    "#     validation_split=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0524ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##disease class\n",
    "# dis_class=train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3998b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in train_ds.take(1):\n",
    "#     print(type(x), x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865eb25",
   "metadata": {},
   "source": [
    "## Alternative way to split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_dataset)*TRAIN_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=raw_dataset.take(550)\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_data=raw_dataset.skip(550)\n",
    "len(rest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_dataset)*VAL_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0333563",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_dataset)*TEST_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=rest_data.take(118)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2de382",
   "metadata": {},
   "outputs": [],
   "source": [
    "val=rest_data.skip(118)\n",
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe196714",
   "metadata": {},
   "outputs": [],
   "source": [
    "##user define function to perform data spliting\n",
    "def train_test_partition(data, train_split=0.80, test_split=0.1, val_split=0.1, shuffle=True, shuffle_size=100):\n",
    "   total_data= len(data)\n",
    "\n",
    "   if shuffle:\n",
    "      data = data.shuffle(shuffle_size, SEED)\n",
    "    \n",
    "   ## train, test, validation size based on the given data and splitting size\n",
    "   train_size=int(total_data*train_split)\n",
    "   val_size=int(total_data*val_split)\n",
    "   \n",
    "   ##splitted data\n",
    "   train_set=data.take(train_size)\n",
    "   val_set=data.skip(train_size).take(val_size)\n",
    "   test_set=data.skip(train_size).skip(val_size)\n",
    "\n",
    "   ## return the train, test and validation set\n",
    "   return train_set, test_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calling the function to extract train, test, validation data \n",
    "train_data, test_data, val_data = train_test_partition(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking \n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc81ff4",
   "metadata": {},
   "source": [
    "<h2 style='text-align:center; font-family:Geogeria; color:white'><b>Display Images</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##print images from the train dataset\n",
    "## taking 1st batch only take(1)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for image,labels in raw_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax=plt.subplot(3,3,i+1)\n",
    "        plt.imshow(np.array(image[i]).astype('uint8'))\n",
    "        plt.title(class_name[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of class\n",
    "##class name\n",
    "class_name=raw_dataset.class_names\n",
    "n_classes=len(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model building\n",
    "##image input size\n",
    "input_shapes=(BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
    "\n",
    "##my model architecture\n",
    "model= Sequential([\n",
    "    ##image resizing\n",
    "    layers.Resizing(IMG_HEIGHT,IMG_WIDTH),\n",
    "    ##normalisation layer\n",
    "    layers.Rescaling(1./255),\n",
    "    ##randomly flip the images\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    ##random rotation\n",
    "    layers.RandomRotation(0.2),\n",
    "\n",
    "    ##first convolutional layer\n",
    "    layers.Conv2D(16, (3,3), activation='relu', padding='same', input_shape=input_shapes),\n",
    "    ##pooling layer\n",
    "    layers.MaxPooling2D(),\n",
    "    ##2nd convolutional layer\n",
    "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    ##3rd convolutional layer\n",
    "    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    ##convert into a vector\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    ##output layer\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "#the model parameters, use the .build() method to see trainable parameters otherwise it won't be needed\n",
    "model.build(input_shape=input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##helps to check trainable parameter\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1fa01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##compile the model with optimizers\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74049f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "##training the model\n",
    "model.fit(train_data,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          validation_data=val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
